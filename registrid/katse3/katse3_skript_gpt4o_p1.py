#Kood registrite t√∂√∂r√ºhma 3. katse tarvis. Eesm√§rk on OpenAI mudelile kaasa anda korpusest andmed, mida ta anal√º√ºsima peab. S√µna antakse ilma t√§henduseta.
#Kui kontekst on liiga suur, siis see vektoriseeritakse.
#Autor: Eleri Aedmaa


import os
import csv
import openai
import pickle
import faiss
import tiktoken
import pandas as pd
import re
import time
from typing import List
from sentence_transformers import SentenceTransformer

# --- Konfiguratsioon ---
client = openai.OpenAI()
MODEL = "gpt-4o"
EMBED_MODEL = SentenceTransformer("intfloat/multilingual-e5-base")
DATA_FOLDER = "contexts"
OUTPUT_FOLDER = "vastused_v2"
FINAL_CSV = "vastused_koond_v2.csv"

os.makedirs(OUTPUT_FOLDER, exist_ok=True)
os.makedirs("vector_cache", exist_ok=True)

# --- Abi funktsioonid ---
def tokenize_length(text: str, model=MODEL):
    enc = tiktoken.encoding_for_model(model)
    return len(enc.encode(text))

def build_faiss_index(chunks: List[str]):
    passages = [f"passage: {chunk}" for chunk in chunks]
    embeddings = EMBED_MODEL.encode(passages, show_progress_bar=False)
    index = faiss.IndexFlatL2(embeddings.shape[1])
    index.add(embeddings)
    return index, embeddings

def save_index(index, chunks, path):
    faiss.write_index(index, f"{path}.faiss")
    with open(f"{path}_chunks.pkl", "wb") as f:
        pickle.dump(chunks, f)

def load_index(path):
    index = faiss.read_index(f"{path}.faiss")
    with open(f"{path}_chunks.pkl", "rb") as f:
        chunks = pickle.load(f)
    return index, chunks

def ensure_index_exists(word: str, full_lines: List[str]):
    index_path = os.path.join("vector_cache", word)
    if os.path.exists(index_path + ".faiss") and os.path.exists(index_path + "_chunks.pkl"):
        return load_index(index_path)
    
    index, _ = build_faiss_index(full_lines)
    save_index(index, full_lines, index_path)
    return index, full_lines

def get_relevant_chunks_max(query: str, chunks: List[str], index, max_k=None):
    query_vec = EMBED_MODEL.encode([f"query: {query}"], show_progress_bar=False)
    D, I = index.search(query_vec, len(chunks))
    sorted_chunks = [chunks[i] for i in I[0]]
    if max_k:
        return sorted_chunks[:max_k]
    return sorted_chunks

def get_completion(prompt: str, context: str) -> str:
    response = client.chat.completions.create(
        model=MODEL,
        messages=[
            {"role": "system", "content": prompt},
            {"role": "user", "content": context}
        ],
        max_tokens=2000,
        temperature=0.1
    )
    return response.choices[0].message.content

def sanitize_filename(text):
    return re.sub(r'[<>:"/\\|?*]', '_', text)[:100]

# --- Uus prompt s√µna anal√º√ºsimiseks ---
def create_analysis_prompt(word: str):
    return f"""Oled eesti keele s√µnaraamatu koostaja. Sinu √ºlesanne on anal√º√ºsida s√µna ‚Äû{word}" kasutust etteantud tekstimaterjalis ja otsustada, kas selle t√§hendustele tuleks lisada registrim√§rgend.

Vasta j√§rgmistele k√ºsimustele, tuginedes ainult etteantud materjalile. Ole l√ºhike ja konkreetne:

1. Nimeta s√µna ‚Äû{word}" k√µik t√§hendused, mida etteantud tekstides n√§ed.
2. Iga t√§henduse juurde lisa, mitmes lauses s√µna selles t√§henduses esineb.
3. Too iga t√§henduse kohta etteantud materjalist kuni 10 n√§itelauset. Kui neid on andmetes v√§hem, siis too nii palju, kui leidub.
4. Otsusta s√µna iga t√§henduse kohta, kas seda kasutatakse pigem informaalsetes v√µi neutraalsetes/formaalsetes tekstides? Kui sa ei oska eristust teha v√µi see ei tule selgelt esile, siis √ºtle, et ‚Äûei kohaldu". Palun p√µhjenda oma valikut.
5. Kui m√µnda t√§hendust kasutatakse pigem informaalsetes tekstides, siis vali sellele sobiv registrim√§rgend j√§rgmistest:
‚Ä¢ halvustav
‚Ä¢ harv
‚Ä¢ k√µnekeelne
‚Ä¢ lastekeelne
‚Ä¢ luulekeelne
‚Ä¢ murdekeelne
‚Ä¢ rahvakeelne
‚Ä¢ stiilitundlik
‚Ä¢ unars√µna
‚Ä¢ vananenud
‚Ä¢ vulgaarne

Iga valiku korral p√µhjenda, miks just see m√§rgend sobib. Igal informaalsel t√§hendusel peab olema v√§hemalt √ºks m√§rgend. Kui sobib mitu, too mitu.

OLULINE: P√§rast k√ºsimustele vastamist anna oma vastused T√ÑPSELT j√§rgmises struktureeritud formaadis:

VASTUS||T√ÑHENDUSED: [t√§hendus1; t√§hendus2; t√§hendus3]||ESINEMISED: [t√§hendus1: X lauses; t√§hendus2: Y lauses]||N√ÑITED: [T√ÑHENDUS1: n√§ide1 | n√§ide2 | n√§ide3; T√ÑHENDUS2: n√§ide1 | n√§ide2]||REGISTRID: [T√ÑHENDUS1: informaalsetes/neutraalsetes-formaalsetes/ei-kohaldu; T√ÑHENDUS2: informaalsetes/neutraalsetes-formaalsetes/ei-kohaldu]||P√ïHJENDUSED: [T√ÑHENDUS1: p√µhjendus1; T√ÑHENDUS2: p√µhjendus2]||M√ÑRGENDID: [T√ÑHENDUS1: m√§rgend1, m√§rgend2 v√µi ei-kohaldu; T√ÑHENDUS2: m√§rgend1 v√µi ei-kohaldu]||M√ÑRGENDITE-P√ïHJENDUS: [T√ÑHENDUS1: m√§rgend1: p√µhjendus1, m√§rgend2: p√µhjendus2; T√ÑHENDUS2: ei-kohaldu]||L√ïPP"""

# --- Uus parsimise funktsioon ---
def parse_analysis_response(txt, word):
    result = {
        "S√µna": word,
        "T√§hendused": "",
        "Esinemiste arv": "",
        "N√§ited": "",
        "Tekstiregistrid": "",
        "Registri p√µhjendused": "",
        "Registrim√§rgendid": "",
        "M√§rgendite p√µhjendused": ""
    }
    
    # Otsime struktureeritud vastust
    structured_match = re.search(r'VASTUS\|\|(.*?)\|\|L√ïPP', txt, re.DOTALL)
    
    if structured_match:
        print("   ‚úÖ Struktureeritud vastus leitud")
        structured_text = structured_match.group(1)
        parts = structured_text.split('||')
        
        for part in parts:
            part = part.strip()
            
            if part.startswith('T√ÑHENDUSED:'):
                tahendused_text = part.replace('T√ÑHENDUSED:', '').strip()
                if tahendused_text:
                    tahendused = [t.strip() for t in tahendused_text.split(';') if t.strip()]
                    result["T√§hendused"] = ' | '.join([f"{i+1}. {t}" for i, t in enumerate(tahendused)])
                    
            elif part.startswith('ESINEMISED:'):
                esinemised_text = part.replace('ESINEMISED:', '').strip()
                result["Esinemiste arv"] = esinemised_text
                
            elif part.startswith('N√ÑITED:'):
                naited_text = part.replace('N√ÑITED:', '').strip()
                result["N√§ited"] = naited_text
                
            elif part.startswith('REGISTRID:'):
                registrid_text = part.replace('REGISTRID:', '').strip()
                result["Tekstiregistrid"] = registrid_text
                
            elif part.startswith('P√ïHJENDUSED:'):
                pohjendused_text = part.replace('P√ïHJENDUSED:', '').strip()
                result["Registri p√µhjendused"] = pohjendused_text
                
            elif part.startswith('M√ÑRGENDID:'):
                margendid_text = part.replace('M√ÑRGENDID:', '').strip()
                result["Registrim√§rgendid"] = margendid_text
                
            elif part.startswith('M√ÑRGENDITE-P√ïHJENDUS:'):
                margendite_pohjendused_text = part.replace('M√ÑRGENDITE-P√ïHJENDUS:', '').strip()
                result["M√§rgendite p√µhjendused"] = margendite_pohjendused_text
    
    else:
        print("   ‚ö†Ô∏è Struktureeritud vastust ei leitud, kasutame vaba teksti parsimist")
        
        # Vaba teksti parsimise loogika (lihtsustatud versioon)
        # Otsime t√§hendusi
        tahendused_match = re.search(r'1\.\s*([^2]*?)(?=2\.|$)', txt, re.DOTALL)
        if tahendused_match:
            tahendused_text = tahendused_match.group(1).strip()
            # Lihtne heuristika - otsime loetelu m√§rke
            tahendused_lines = [line.strip() for line in tahendused_text.split('\n') if line.strip()]
            tahendused = []
            for line in tahendused_lines:
                if re.match(r'^[‚Ä¢\-\*]\s*|^\d+\.\s*|^[a-z]\)\s*', line):
                    tahendused.append(re.sub(r'^[‚Ä¢\-\*]\s*|^\d+\.\s*|^[a-z]\)\s*', '', line))
            if tahendused:
                result["T√§hendused"] = ' | '.join([f"{i+1}. {t}" for i, t in enumerate(tahendused)])
        
        # Muud v√§ljad j√§tame t√ºhjaks v√µi m√§rgime "vaba tekst"
        result["Esinemiste arv"] = "vaba tekst - vt toorvastust"
        result["N√§ited"] = "vaba tekst - vt toorvastust"
        result["Tekstiregistrid"] = "vaba tekst - vt toorvastust"
        result["Registri p√µhjendused"] = "vaba tekst - vt toorvastust"
        result["Registrim√§rgendid"] = "vaba tekst - vt toorvastust"
        result["M√§rgendite p√µhjendused"] = "vaba tekst - vt toorvastust"
    
    # Vaikev√§√§rtused t√ºhjadele v√§ljadele
    for key in result:
        if not result[key]:
            result[key] = "ei m√§√§ratletud"
    
    return result

# --- S√µna t√∂√∂tlemise funktsioon ---
def process_word_analysis(word: str):
    context_path = os.path.join(DATA_FOLDER, f"{word}_full_context_only.txt")
    if not os.path.exists(context_path):
        print(f"‚õî Puudub kontekstifail: {context_path}")
        return None

    with open(context_path, "r", encoding="utf-8") as f:
        lines = [line.strip() for line in f if line.strip()]

    full_text = "\n".join(lines)
    if tokenize_length(full_text) < 120000:
        context = full_text
        print(f"üìÑ Kasutan t√§ielikku konteksti ({len(lines)} rida)")
    else:
        print(f"‚ÑπÔ∏è Fail on suur ‚Äì kasutatakse embedding-p√µhist l√µiguvalikut ({word})")
        index, chunks = ensure_index_exists(word, lines)
        relevant_chunks = get_relevant_chunks_max(word, chunks, index, max_k=150)
        context = "\n---\n".join(relevant_chunks)
        print(f"üìÑ Kasutan {len(relevant_chunks)} k√µige relevantsemast l√µiku")

    prompt = create_analysis_prompt(word)

    try:
        reply = get_completion(prompt, context)
        
        # Prindime mudeli toorvastuse
        print("\n" + "="*80)
        print(f"ü§ñ MUDELI VASTUS s√µnale '{word}':")
        print("="*80)
        print(reply)
        print("="*80)
        
        # Salvesta toorvastus faili
        safe_word = sanitize_filename(word)
        out_path = os.path.join(OUTPUT_FOLDER, f"{safe_word}_analysis.txt")
        with open(out_path, "w", encoding="utf-8") as out_f:
            out_f.write(reply)
        
        # Parsime vastuse
        parsed_result = parse_analysis_response(reply, word)
        
        # Prindime parsimise tulemuse
        print(f"\nüìä PARSITUD TULEMUS:")
        print(f"   üìù T√§hendused: {parsed_result['T√§hendused'][:100]}{'...' if len(parsed_result['T√§hendused']) > 100 else ''}")
        print(f"   üìä Esinemised: {parsed_result['Esinemiste arv'][:100]}{'...' if len(parsed_result['Esinemiste arv']) > 100 else ''}")
        print(f"   üìã Registrid: {parsed_result['Tekstiregistrid'][:100]}{'...' if len(parsed_result['Tekstiregistrid']) > 100 else ''}")
        print(f"   üè∑Ô∏è M√§rgendid: {parsed_result['Registrim√§rgendid'][:100]}{'...' if len(parsed_result['Registrim√§rgendid']) > 100 else ''}")
        
        print(f"‚úÖ {word} ‚Äî Anal√º√ºs l√µpetatud\n")
        
        return parsed_result

    except Exception as e:
        print(f"‚ùå Viga s√µnaga {word}: {e}")
        return None

# --- P√µhiprogramm ---
def main():
    all_rows = []
    
    # Loeme sisend_2.tsv faili (ainult s√µnad)
    with open("sisend_2.tsv", newline="", encoding="utf-8") as csvfile:
        reader = csv.reader(csvfile)
        next(reader, None)  # j√§ta p√§is vahele, kui on
        words = [row[0].strip() for row in reader if len(row) >= 1 and row[0].strip()]
    
    for i, word in enumerate(words, 1):
        print(f"\n{'='*60}")
        print(f"üìù ANAL√ú√úSIN ({i}/{len(words)}): '{word}'")
        print(f"{'='*60}")
        
        result = process_word_analysis(word)
        if result:
            all_rows.append(result)
        else:
            # Lisa t√ºhi rida j√§rjekorra s√§ilitamiseks
            print(f"‚ö†Ô∏è Lisame t√ºhja rea j√§rjekorra s√§ilitamiseks")
            all_rows.append({
                "S√µna": word,
                "T√§hendused": "t√∂√∂tlemata",
                "Esinemiste arv": "kontekstifail puudub",
                "N√§ited": "ei saadaval",
                "Tekstiregistrid": "ei m√§√§ratletud",
                "Registri p√µhjendused": "ei saadaval",
                "Registrim√§rgendid": "ei kohaldu",
                "M√§rgendite p√µhjendused": "ei saadaval"
            })
        
        time.sleep(0.5)  # V√§ike paus

    # Salvesta CSV
    fieldnames = [
        "S√µna", "T√§hendused", "Esinemiste arv", "N√§ited", 
        "Tekstiregistrid", "Registri p√µhjendused", 
        "Registrim√§rgendid", "M√§rgendite p√µhjendused"
    ]

    with open(FINAL_CSV, "w", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames, delimiter=";", quoting=csv.QUOTE_ALL)
        writer.writeheader()
        for row in all_rows:
            writer.writerow(row)

    print(f"\n‚úÖ L√µplik fail salvestatud: {FINAL_CSV}")
    print(f"üìä Kokku anal√º√ºsitud {len(all_rows)} s√µna")

    # Statistika
    to√∂tletud = len([row for row in all_rows if row["T√§hendused"] != "t√∂√∂tlemata"])
    to√∂tlemata = len(all_rows) - to√∂tletud

    print(f"\nüìà Anal√º√ºsi statistika:")
    print(f"  Edukalt t√∂√∂deldud: {to√∂tletud}")
    print(f"  T√∂√∂tlemata (puuduvad failid): {to√∂tlemata}")

if __name__ == "__main__":
    main()