#Kood, registrite t√∂√∂r√ºhma 3. katse tarvis. Eesm√§rk on OpenAI mudelile kaasa anda korpusest andmed, mida ta anal√º√ºsima peab. Igale anal√º√ºsitavale s√µnale antakse kaasa ka t√§hendus.
#Kui kontekst on liiga suur, siis see vektoriseeritakse.
#Autor: Eleri Aedmaa

import os
import csv
import openai
import pickle
import faiss
import tiktoken
import pandas as pd
import re
import time
from typing import List, Dict, Any
from sentence_transformers import SentenceTransformer

# --- Konfiguratsioon ---
client = openai.OpenAI()
MODEL = "gpt-4.1-2025-04-14"
EMBED_MODEL = SentenceTransformer("intfloat/multilingual-e5-base")
DATA_FOLDER = "contexts"
OUTPUT_FOLDER = "vastused"
FINAL_CSV = "vastused_koond.csv"

os.makedirs(OUTPUT_FOLDER, exist_ok=True)
os.makedirs("vector_cache", exist_ok=True)

# --- Abi funktsioonid ---
def tokenize_length(text: str):
    enc = tiktoken.get_encoding("cl100k_base")
    return len(enc.encode(text))


def build_faiss_index(chunks: List[str]):
    passages = [f"passage: {chunk}" for chunk in chunks]
    embeddings = EMBED_MODEL.encode(passages, show_progress_bar=False)
    index = faiss.IndexFlatL2(embeddings.shape[1])
    index.add(embeddings)
    return index, embeddings

def save_index(index, chunks, path):
    faiss.write_index(index, f"{path}.faiss")
    with open(f"{path}_chunks.pkl", "wb") as f:
        pickle.dump(chunks, f)

def load_index(path):
    index = faiss.read_index(f"{path}.faiss")
    with open(f"{path}_chunks.pkl", "rb") as f:
        chunks = pickle.load(f)
    return index, chunks

def ensure_index_exists(word: str, full_lines: List[str]):
    index_path = os.path.join("vector_cache", word)
    if os.path.exists(index_path + ".faiss") and os.path.exists(index_path + "_chunks.pkl"):
        return load_index(index_path)
    
    index, _ = build_faiss_index(full_lines)
    save_index(index, full_lines, index_path)
    return index, full_lines

def get_relevant_chunks_max(query: str, chunks: List[str], index, max_k=None):
    query_vec = EMBED_MODEL.encode([f"query: {query}"], show_progress_bar=False)
    D, I = index.search(query_vec, len(chunks))
    sorted_chunks = [chunks[i] for i in I[0]]
    if max_k:
        return sorted_chunks[:max_k]
    return sorted_chunks

def get_completion(prompt: str, context: str) -> str:
    response = client.chat.completions.create(
        model=MODEL,
        messages=[
            {"role": "system", "content": prompt},
            {"role": "user", "content": context}
        ],
        max_tokens=16000,
        temperature=0.1
    )
    return response.choices[0].message.content

def sanitize_filename(text):
    return re.sub(r'[<>:"/\\|?*]', '_', text)[:100]

# --- Prompt s√µna ja t√§henduse anal√º√ºsimiseks ---
def create_definition_analysis_prompt(word: str, definition: str):
    return f"""Oled eesti keele s√µnaraamatu koostaja. Sinu √ºlesanne on hinnata, kas s√µnale ‚Äû{word}" tuleb t√§henduses ‚Äû{definition}" lisada registrim√§rgend. Vasta ainult etteantud konteksti p√µhjal.

Vasta j√§rgmistele k√ºsimustele:

1. Otsusta s√µna ‚Äû{word}" t√§henduse ‚Äû{definition}" kohta, kas seda kasutatakse pigem informaalsetes v√µi neutraalsetes/formaalsetes tekstides? Kui sa ei oska eristust teha v√µi see ei tule selgelt esile, siis √ºtle, et ‚Äûei kohaldu". Palun p√µhjenda oma valikut 5-10 lausega.

2. Kui valid ‚Äûei kohaldu", siis ja ainult siis vaata enda treeningandmetesse ja otsusta selle p√µhjal, kas seda kasutatakse pigem informaalsetes v√µi neutraalsetes/formaalsetes tekstides. Palun p√µhjenda oma valikut 5-10 lausega.

3. Nimeta s√µna ‚Äû{word}" erinevate t√§henduste arv.

4. Iga t√§henduse juurde lisa, kas s√µna on selles t√§henduses sage, keskmine v√µi harv. Sagedusr√ºhm vali v√µrdluses s√µna teiste t√§hendustega.

5. Too 3 n√§idet etteantud materjalist, kus s√µna ‚Äû{word}" esineb just selles t√§henduses. Kui n√§iteid on v√§hem, too nii palju, kui leidub.

6. Kui valisid, et s√µna selles t√§henduses esineb pigem *informaalsetes* tekstides, siis:
   - Millise registrim√§rgendeist sellele t√§hendusele lisaksid? (vali v√§hemalt √ºks, v√µid valida mitu):
     ‚Ä¢ halvustav (n√§iteks ajuh√§lvik, debiilik, inimr√§mps)
     ‚Ä¢ harv (n√§iteks ahvatama, m√µistamisi, siinap)
     ‚Ä¢ k√µnekeelne (n√§iteks igastahes, nokats, √§ra flippima)
     ‚Ä¢ lastekeelne (n√§iteks j√§nku, k√§tu, nuku)
     ‚Ä¢ luulekeelne (n√§iteks ehavalu, koidukuld, meeleheit)
     ‚Ä¢ murdekeelne (n√§iteks h√§mmelgas, j√µ√µrdlik, kidelema)
     ‚Ä¢ rahvakeelne (n√§iteks heinakuu, viinakuu, m√§nniseen)
     ‚Ä¢ stiilitundlik (n√§iteks armastet, kirjutet, seitung)
     ‚Ä¢ unars√µna (n√§iteks absurdum, √∂√∂p)
     ‚Ä¢ vananenud (n√§iteks automobiil, drogist)
     ‚Ä¢ vulgaarne (n√§iteks hoorapoeg, koinima, munn)
   - M√§rgend ‚Äûharv" vali iga kord, kui t√§hendust leidub etteantud tekstimaterjalis v√§he
   - P√µhjenda iga m√§rgendivalikut 5-10 lausega.

OLULINE: P√§rast k√ºsimustele vastamist anna oma vastused T√ÑPSELT j√§rgmises struktureeritud formaadis parsimiseks:

--- STRUKTUREERITUD VASTUS ALGAB ---
S√ïNA: {word}
T√ÑHENDUS: {definition}
TEKSTIREGISTER: informaalsetes/neutraalsetes-formaalsetes/ei-kohaldu
REGISTRI-P√ïHJENDUS: [5-10 lauseline p√µhjendus]
TREENINGANDMETE-P√ïHJENDUS: [5-10 lauseline p√µhjendus v√µi ei-kohaldu]
T√ÑHENDUSTE-ARV: [number]
SAGEDUS: sage/keskmine/harv
N√ÑITED: N√§ide 1|N√§ide 2|N√§ide 3
REGISTRIM√ÑRK: halvustav,k√µnekeelne v√µi ei-kohaldu
M√ÑRGENDI-P√ïHJENDUS: [5-10 lauseline p√µhjendus iga m√§rgendi kohta v√µi ei-kohaldu]
--- STRUKTUREERITUD VASTUS L√ïPEB ---"""

# --- Parsimise funktsioon ---
def parse_definition_analysis_response(txt: str, word: str, definition: str) -> Dict[str, Any]:
    """
    Tagastab √ºhe t√§henduse anal√º√ºsi tulemuse
    """
    result = {
        "S√µna": word,
        "T√§hendus": definition,
        "Tekstiregister": "ei m√§√§ratletud",
        "Registri p√µhjendus": "ei m√§√§ratletud",
        "Treeningandmete p√µhjendus": "ei-kohaldu",
        "T√§henduste arv kokku": 0,
        "Sagedus": "ei m√§√§ratletud",
        "N√§ited": "ei leitud",
        "Registrim√§rk": "ei-kohaldu",
        "M√§rgendi p√µhjendus": "ei-kohaldu"
    }
    
    try:
        # Otsime struktureeritud vastust m√§rgendite vahelt
        structured_match = re.search(r'--- STRUKTUREERITUD VASTUS ALGAB ---(.*?)--- STRUKTUREERITUD VASTUS L√ïPEB ---', txt, re.DOTALL)
        
        if not structured_match:
            print("   ‚ö†Ô∏è Struktureeritud vastust ei leitud, parsime kogu teksti")
            structured_text = txt
        else:
            structured_text = structured_match.group(1)
        
        lines = structured_text.split('\n')
        data = {}
        
        for line in lines:
            line = line.strip()
            if ':' in line and not line.startswith('http'):
                key, value = line.split(':', 1)
                key = key.strip()
                value = value.strip()
                data[key] = value
        
        # Parsime andmed
        if 'S√ïNA' in data:
            result["S√µna"] = data['S√ïNA']
        
        if 'T√ÑHENDUS' in data:
            result["T√§hendus"] = data['T√ÑHENDUS']
        
        if 'TEKSTIREGISTER' in data:
            result["Tekstiregister"] = data['TEKSTIREGISTER']
        
        if 'REGISTRI-P√ïHJENDUS' in data:
            result["Registri p√µhjendus"] = data['REGISTRI-P√ïHJENDUS']
        
        if 'TREENINGANDMETE-P√ïHJENDUS' in data:
            result["Treeningandmete p√µhjendus"] = data['TREENINGANDMETE-P√ïHJENDUS']
        
        if 'T√ÑHENDUSTE-ARV' in data:
            result["T√§henduste arv kokku"] = data['T√ÑHENDUSTE-ARV']
        
        if 'SAGEDUS' in data:
            result["Sagedus"] = data['SAGEDUS']
        
        if 'N√ÑITED' in data:
            result["N√§ited"] = data['N√ÑITED'].replace('|', ' | ')
        
        if 'REGISTRIM√ÑRK' in data:
            result["Registrim√§rk"] = data['REGISTRIM√ÑRK']
        
        if 'M√ÑRGENDI-P√ïHJENDUS' in data:
            result["M√§rgendi p√µhjendus"] = data['M√ÑRGENDI-P√ïHJENDUS']
        
        print(f"   ‚úÖ T√§hendus: {result['T√§hendus'][:50]}{'...' if len(result['T√§hendus']) > 50 else ''}")
        print(f"      üìä Register: {result['Tekstiregister']}")
        print(f"      üîç Registri p√µhjendus: {result['Registri p√µhjendus'][:100]}{'...' if len(result['Registri p√µhjendus']) > 100 else ''}")
        print(f"      üè∑Ô∏è M√§rgend: {result['Registrim√§rk']}")
        if result["M√§rgendi p√µhjendus"] != "ei-kohaldu":
            print(f"      üìù M√§rgendi p√µhjendus: {result['M√§rgendi p√µhjendus'][:100]}{'...' if len(result['M√§rgendi p√µhjendus']) > 100 else ''}")
    
    except Exception as e:
        print(f"   ‚ö†Ô∏è Parsimise viga: {e}")
        import traceback
        traceback.print_exc()
        result["T√§hendus"] = "parsimise viga"
    
    return result

# --- S√µna ja t√§henduse t√∂√∂tlemise funktsioon ---
def process_definition_analysis(word: str, definition: str):
    context_path = os.path.join(DATA_FOLDER, f"{word}_full_context_only.txt")
    if not os.path.exists(context_path):
        print(f"‚õî Puudub kontekstifail: {context_path}")
        return None

    with open(context_path, "r", encoding="utf-8") as f:
        lines = [line.strip() for line in f if line.strip()]

    full_text = "\n".join(lines)
    if tokenize_length(full_text) < 120000:
        context = full_text
        print(f"üìÑ Kasutan t√§ielikku konteksti ({len(lines)} rida)")
    else:
        print(f"‚ÑπÔ∏è Fail on suur ‚Äì kasutatakse embedding-p√µhist l√µiguvalikut ({word})")
        index, chunks = ensure_index_exists(word, lines)
        # Otsime relevantset sisu nii s√µna kui t√§henduse p√µhjal
        query = f"{word} {definition}"
        relevant_chunks = get_relevant_chunks_max(query, chunks, index, max_k=150)
        context = "\n---\n".join(relevant_chunks)
        print(f"üìÑ Kasutan {len(relevant_chunks)} k√µige relevantsemast l√µiku")

    prompt = create_definition_analysis_prompt(word, definition)

    try:
        reply = get_completion(prompt, context)
        
        # Prindime mudeli toorvastuse
        print("\n" + "="*80)
        print(f"ü§ñ MUDELI VASTUS s√µnale '{word}' t√§henduses '{definition[:50]}{'...' if len(definition) > 50 else ''}':")
        print("="*80)
        print(reply)
        print("="*80)
        
        # Salvesta toorvastus faili
        safe_word = sanitize_filename(word)
        safe_definition = sanitize_filename(definition)
        out_path = os.path.join(OUTPUT_FOLDER, f"{safe_word}_{safe_definition}_analysis.txt")
        with open(out_path, "w", encoding="utf-8") as out_f:
            out_f.write(reply)
        
        # Parsime vastuse
        parsed_result = parse_definition_analysis_response(reply, word, definition)
        
        print(f"‚úÖ {word} (t√§hendus: {definition[:30]}{'...' if len(definition) > 30 else ''}) ‚Äî Anal√º√ºs l√µpetatud\n")
        
        return parsed_result

    except Exception as e:
        print(f"‚ùå Viga s√µnaga {word}, t√§hendus {definition}: {e}")
        return None

# --- P√µhiprogramm ---
def main():
    all_rows = []
    
    # Loeme sisendandmeid (eeldame, et fail sisaldab veerge: s√µna, t√§hendus)
    input_file = "sisend.tsv"  # Muuda faili nime vastavalt vajadusele
    
    if not os.path.exists(input_file):
        print(f"‚õî Sisend fail '{input_file}' puudub!")
        print("Palun loo fail j√§rgmise struktuuriga:")
        print("s√µna<TAB>t√§hendus")
        print("kits<TAB>koduloom")
        print("kits<TAB>Hiina sodiaagim√§rk")
        return
    
    with open(input_file, newline="", encoding="utf-8") as csvfile:
        reader = csv.reader(csvfile, delimiter='\t')
        next(reader, None)  # j√§ta p√§is vahele, kui on
        word_definitions = []
        for row in reader:
            if len(row) >= 2 and row[0].strip() and row[1].strip():
                word_definitions.append((row[0].strip(), row[1].strip()))
    
    for i, (word, definition) in enumerate(word_definitions, 1):
        print(f"\n{'='*60}")
        print(f"üìù ANAL√ú√úSIN ({i}/{len(word_definitions)}): '{word}' - '{definition[:50]}{'...' if len(definition) > 50 else ''}'")
        print(f"{'='*60}")
        
        result = process_definition_analysis(word, definition)
        if result:
            all_rows.append(result)
        else:
            # Lisa t√ºhi rida j√§rjekorra s√§ilitamiseks
            print(f"‚ö†Ô∏è Lisame t√ºhja rea j√§rjekorra s√§ilitamiseks")
            all_rows.append({
                "S√µna": word,
                "T√§hendus": definition,
                "Tekstiregister": "ei m√§√§ratletud",
                "Registri p√µhjendus": "kontekstifail puudub",
                "Treeningandmete p√µhjendus": "ei saadaval",
                "T√§henduste arv kokku": 0,
                "Sagedus": "ei m√§√§ratletud",
                "N√§ited": "ei saadaval",
                "Registrim√§rk": "ei kohaldu",
                "M√§rgendi p√µhjendus": "ei saadaval"
            })
        
        time.sleep(0.5)  # V√§ike paus

    # Salvesta CSV
    fieldnames = [
        "S√µna", "T√§hendus", "Tekstiregister", "Registri p√µhjendus", 
        "Treeningandmete p√µhjendus", "T√§henduste arv kokku", "Sagedus", 
        "N√§ited", "Registrim√§rk", "M√§rgendi p√µhjendus"
    ]

    with open(FINAL_CSV, "w", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames, delimiter=";", quoting=csv.QUOTE_ALL)
        writer.writeheader()
        for row in all_rows:
            writer.writerow(row)

    print(f"\n‚úÖ L√µplik fail salvestatud: {FINAL_CSV}")
    print(f"üìä Kokku anal√º√ºsitud ridu: {len(all_rows)}")

    # Statistika
    unikaalsed_s√µnad = len(set(row["S√µna"] for row in all_rows))
    to√∂tletud_read = len([row for row in all_rows if row["T√§hendus"] != "parsimise viga"])
    to√∂tlemata_read = len(all_rows) - to√∂tletud_read

    print(f"\nüìà Anal√º√ºsi statistika:")
    print(f"  Kokku s√µnu: {unikaalsed_s√µnad}")
    print(f"  Kokku t√§hendusi: {len(all_rows)}")
    print(f"  Edukalt t√∂√∂deldud t√§hendusi: {to√∂tletud_read}")
    print(f"  T√∂√∂tlemata t√§hendusi: {to√∂tlemata_read}")

if __name__ == "__main__":
    main()