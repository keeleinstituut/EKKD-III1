#Kood, registrite t√∂√∂r√ºhma 3. katse tarvis. Eesm√§rk on Google'i mudelile kaasa anda korpusest andmed, mida ta anal√º√ºsima peab. Igale anal√º√ºsitavale s√µnale antakse kaasa ka t√§hendus.
#Kui kontekst on liiga suur, siis see vektoriseeritakse.
#Autor: Eleri Aedmaa
import os
import csv
import google.generativeai as genai
import pickle
import faiss
import time
import re
from typing import List, Dict, Any
from sentence_transformers import SentenceTransformer
from google.api_core import exceptions

# --- Konfiguratsioon ---
try:
    genai.configure(api_key=os.environ["GOOGLE_API_KEY"])
except KeyError:
    print("VIGA: GOOGLE_API_KEY keskkonnamuutja pole seadistatud.")
    exit()

# Mudelite ja parameetrite seadistus
MODEL_NAME = "gemini-2.5-pro"
EMBED_MODEL = SentenceTransformer("intfloat/multilingual-e5-base")

DATA_FOLDER = "contexts"
OUTPUT_FOLDER = "vastused"
FINAL_CSV = "vastused_koond.csv"
VECTOR_CACHE_FOLDER = "vector_cache"

# Kontrollide seadistus
MAX_CONTEXT_LINES = 10000
NUM_RELEVANT_CHUNKS = 150

os.makedirs(OUTPUT_FOLDER, exist_ok=True)
os.makedirs(VECTOR_CACHE_FOLDER, exist_ok=True)

# --- VEKTOR-FUNKTSIOONID ---


def build_faiss_index(chunks: List[str]):
    passages = [f"passage: {chunk}" for chunk in chunks]
    embeddings = EMBED_MODEL.encode(passages, show_progress_bar=True)
    index = faiss.IndexFlatL2(embeddings.shape[1])
    index.add(embeddings)
    return index, embeddings

def save_index(index, chunks, path):
    faiss.write_index(index, f"{path}.faiss")
    with open(f"{path}_chunks.pkl", "wb") as f:
        pickle.dump(chunks, f)

def load_index(path):
    index = faiss.read_index(f"{path}.faiss")
    with open(f"{path}_chunks.pkl", "rb") as f:
        chunks = pickle.load(f)
    return index, chunks

def ensure_index_exists(word: str, full_lines: List[str]):
    index_path = os.path.join(VECTOR_CACHE_FOLDER, sanitize_filename(word))
    if os.path.exists(index_path + ".faiss") and os.path.exists(index_path + "_chunks.pkl"):
        print("   -> Leidsin vahem√§lust valmis vektoriindeksi, laen selle.")
        return load_index(index_path)
    
    print("   -> Vektoriindeksit ei leitud, loon uue (see v√µib v√µtta aega)...")
    index, _ = build_faiss_index(full_lines)
    save_index(index, full_lines, index_path)
    print("   -> Uus indeks loodud ja salvestatud vahem√§llu.")
    return index, full_lines

def get_relevant_chunks_max(query: str, chunks: List[str], index, max_k=150):
    query_vec = EMBED_MODEL.encode([f"query: {query}"], show_progress_bar=False)
    distances, indices = index.search(query_vec, min(len(chunks), max_k + 10))
    sorted_chunks = [chunks[i] for i in indices[0]]
    unique_chunks = list(dict.fromkeys(sorted_chunks))
    return unique_chunks[:max_k]

# --- √úLEJ√Ñ√ÑNUD ABIFUNKTSIOONID, PROMPTID JA PARSERID ---


def get_completion(prompt: str, context: str) -> str:
    """
    Saadab p√§ringu Gemini mudelile ja tagastab vastuse tekstina.
    """
    generation_config = {
        "temperature": 0.1,
        "max_output_tokens": 16000,
    }

    # Loome mudeli, edastades s√ºsteemiprompti ja genereerimise seaded
    model = genai.GenerativeModel(
        model_name=MODEL_NAME,
        system_instruction=prompt,
        generation_config=generation_config
    )
    
    # Saadame konteksti kui kasutaja s√µnumi
    response = model.generate_content(context)
    
    # Kontrollime, kas vastus blokeeriti turvakaalutlustel
    if not response.parts:
         print("‚ö†Ô∏è Mudeli vastus oli t√ºhi v√µi blokeeritud.")
         return "MUDELI_VASTUS_BLOKEERITUD"
         
    return response.text

def sanitize_filename(text):
    return re.sub(r'[<>:"/\\|?*]', '_', text)[:100]

def create_definition_analysis_prompt(word: str, definition: str):
    return f"""Oled eesti keele s√µnaraamatu koostaja. Sinu √ºlesanne on hinnata, kas s√µnale ‚Äû{word}" tuleb t√§henduses ‚Äû{definition}" lisada registrim√§rgend. Vasta ainult etteantud konteksti p√µhjal.

Vasta j√§rgmistele k√ºsimustele:

1. Otsusta s√µna ‚Äû{word}" t√§henduse ‚Äû{definition}" kohta, kas seda kasutatakse pigem informaalsetes v√µi neutraalsetes/formaalsetes tekstides? Kui sa ei oska eristust teha v√µi see ei tule selgelt esile, siis √ºtle, et ‚Äûei kohaldu". Palun p√µhjenda oma valikut 5-10 lausega.

2. Kui valid ‚Äûei kohaldu", siis ja ainult siis vaata enda treeningandmetesse ja otsusta selle p√µhjal, kas seda kasutatakse pigem informaalsetes v√µi neutraalsetes/formaalsetes tekstides. Palun p√µhjenda oma valikut 5-10 lausega.

3. Nimeta s√µna ‚Äû{word}" erinevate t√§henduste arv.

4. Iga t√§henduse juurde lisa, kas s√µna on selles t√§henduses sage, keskmine v√µi harv. Sagedusr√ºhm vali v√µrdluses s√µna teiste t√§hendustega.

5. Too 3 n√§idet etteantud materjalist, kus s√µna ‚Äû{word}" esineb just selles t√§henduses. Kui n√§iteid on v√§hem, too nii palju, kui leidub.

6. Kui valisid, et s√µna selles t√§henduses esineb pigem *informaalsetes* tekstides, siis:
    - Millise registrim√§rgendeist sellele t√§hendusele lisaksid? (vali v√§hemalt √ºks, v√µid valida mitu):
      ‚Ä¢ halvustav (n√§iteks ajuh√§lvik, debiilik, inimr√§mps)
      ‚Ä¢ harv (n√§iteks ahvatama, m√µistamisi, siinap)
      ‚Ä¢ k√µnekeelne (n√§iteks igastahes, nokats, √§ra flippima)
      ‚Ä¢ lastekeelne (n√§iteks j√§nku, k√§tu, nuku)
      ‚Ä¢ luulekeelne (n√§iteks ehavalu, koidukuld, meeleheit)
      ‚Ä¢ murdekeelne (n√§iteks h√§mmelgas, j√µ√µrdlik, kidelema)
      ‚Ä¢ rahvakeelne (n√§iteks heinakuu, viinakuu, m√§nniseen)
      ‚Ä¢ stiilitundlik (n√§iteks armastet, kirjutet, seitung)
      ‚Ä¢ unars√µna (n√§iteks absurdum, √∂√∂p)
      ‚Ä¢ vananenud (n√§iteks automobiil, drogist)
      ‚Ä¢ vulgaarne (n√§iteks hoorapoeg, koinima, munn)
    - M√§rgend ‚Äûharv" vali iga kord, kui t√§hendust leidub etteantud tekstimaterjalis v√§he
    - P√µhjenda iga m√§rgendivalikut 5-10 lausega.

OLULINE: P√§rast k√ºsimustele vastamist anna oma vastused T√ÑPSELT j√§rgmises struktureeritud formaadis parsimiseks:

--- STRUKTUREERITUD VASTUS ALGAB ---
S√ïNA: {word}
T√ÑHENDUS: {definition}
TEKSTIREGISTER: informaalsetes/neutraalsetes-formaalsetes/ei-kohaldu
REGISTRI-P√ïHJENDUS: [5-10 lauseline p√µhjendus]
TREENINGANDMETE-P√ïHJENDUS: [5-10 lauseline p√µhjendus v√µi ei-kohaldu]
T√ÑHENDUSTE-ARV: [number]
SAGEDUS: sage/keskmine/harv
N√ÑITED: N√§ide 1|N√§ide 2|N√§ide 3
REGISTRIM√ÑRK: halvustav,k√µnekeelne v√µi ei-kohaldu
M√ÑRGENDI-P√ïHJENDUS: [5-10 lauseline p√µhjendus iga m√§rgendi kohta v√µi ei-kohaldu]
--- STRUKTUREERITUD VASTUS L√ïPEB ---"""

def parse_definition_analysis_response(txt: str, word: str, definition: str) -> Dict[str, Any]:
    """
    Tagastab √ºhe t√§henduse anal√º√ºsi tulemuse.
    Funktsioon ei vaja muudatusi, kuna see t√∂√∂tleb mudeli tekstilist v√§ljundit.
    """
    result = {
        "S√µna": word,
        "T√§hendus": definition,
        "Tekstiregister": "ei m√§√§ratletud",
        "Registri p√µhjendus": "ei m√§√§ratletud",
        "Treeningandmete p√µhjendus": "ei-kohaldu",
        "T√§henduste arv kokku": 0,
        "Sagedus": "ei m√§√§ratletud",
        "N√§ited": "ei leitud",
        "Registrim√§rk": "ei-kohaldu",
        "M√§rgendi p√µhjendus": "ei-kohaldu"
    }
    
    try:
        # Otsime struktureeritud vastust m√§rgendite vahelt
        structured_match = re.search(r'--- STRUKTUREERITUD VASTUS ALGAB ---(.*?)--- STRUKTUREERITUD VASTUS L√ïPEB ---', txt, re.DOTALL)
        
        if not structured_match:
            print("     ‚ö†Ô∏è Struktureeritud vastust ei leitud, parsime kogu teksti")
            structured_text = txt
        else:
            structured_text = structured_match.group(1)
        
        lines = structured_text.split('\n')
        data = {}
        
        for line in lines:
            line = line.strip()
            if ':' in line and not line.startswith('http'):
                key, value = line.split(':', 1)
                key = key.strip()
                value = value.strip()
                data[key] = value
        
        # Parsime andmed
        if 'S√ïNA' in data:
            result["S√µna"] = data['S√ïNA']
        
        if 'T√ÑHENDUS' in data:
            result["T√§hendus"] = data['T√ÑHENDUS']
        
        if 'TEKSTIREGISTER' in data:
            result["Tekstiregister"] = data['TEKSTIREGISTER']
        
        if 'REGISTRI-P√ïHJENDUS' in data:
            result["Registri p√µhjendus"] = data['REGISTRI-P√ïHJENDUS']
        
        if 'TREENINGANDMETE-P√ïHJENDUS' in data:
            result["Treeningandmete p√µhjendus"] = data['TREENINGANDMETE-P√ïHJENDUS']
        
        if 'T√ÑHENDUSTE-ARV' in data:
            result["T√§henduste arv kokku"] = data['T√ÑHENDUSTE-ARV']
        
        if 'SAGEDUS' in data:
            result["Sagedus"] = data['SAGEDUS']
        
        if 'N√ÑITED' in data:
            result["N√§ited"] = data['N√ÑITED'].replace('|', ' | ')
        
        if 'REGISTRIM√ÑRK' in data:
            result["Registrim√§rk"] = data['REGISTRIM√ÑRK']
        
        if 'M√ÑRGENDI-P√ïHJENDUS' in data:
            result["M√§rgendi p√µhjendus"] = data['M√ÑRGENDI-P√ïHJENDUS']
        
        print(f"     ‚úÖ T√§hendus: {result['T√§hendus'][:50]}{'...' if len(result['T√§hendus']) > 50 else ''}")
        print(f"        üìä Register: {result['Tekstiregister']}")
        print(f"        üîç Registri p√µhjendus: {result['Registri p√µhjendus'][:100]}{'...' if len(result['Registri p√µhjendus']) > 100 else ''}")
        print(f"        üè∑Ô∏è  M√§rgend: {result['Registrim√§rk']}")
        if result["M√§rgendi p√µhjendus"] != "ei-kohaldu":
            print(f"        üìù M√§rgendi p√µhjendus: {result['M√§rgendi p√µhjendus'][:100]}{'...' if len(result['M√§rgendi p√µhjendus']) > 100 else ''}")
    
    except Exception as e:
        print(f"     ‚ö†Ô∏è Parsimise viga: {e}")
        import traceback
        traceback.print_exc()
        result["T√§hendus"] = "parsimise viga"
    
    return result

# --- S√ïNA JA T√ÑHENDUSE T√ñ√ñTLEMISE FUNKTSIOON ---
def process_definition_analysis(word: str, definition: str):
    context_path = os.path.join(DATA_FOLDER, f"{word}_full_context_only.txt")
    if not os.path.exists(context_path):
        print(f"‚õî Puudub kontekstifail: {context_path}")
        return None

    with open(context_path, "r", encoding="utf-8") as f:
        lines = [line.strip() for line in f if line.strip()]

    # --- KONTEKSTI VALIKU LOOGIKA ---
    if len(lines) > MAX_CONTEXT_LINES:
        print(f"‚ö†Ô∏è  Kontekstifail on suur ({len(lines)} rida). Kasutan vektorotsingut.")
        index, chunks = ensure_index_exists(word, lines)
        
        # Loome spetsiifilise p√§ringu nii s√µna kui t√§hendusega
        query = f"{word} {definition}"
        print(f"   -> Otsin relevantseid l√µike p√§ringuga: '{query[:100]}...'")

        relevant_chunks = get_relevant_chunks_max(query, chunks, index, max_k=NUM_RELEVANT_CHUNKS)
        context = "\n---\n".join(relevant_chunks)
        print(f"üìÑ Kasutan kontekstina {len(relevant_chunks)} k√µige relevantsemat l√µiku.")
    else:
        print(f"üìÑ Fail on piisavalt v√§ike ({len(lines)} rida). Kasutan t√§ielikku konteksti.")
        context = "\n".join(lines)

    prompt = create_definition_analysis_prompt(word, definition)

    # Vigade k√§sitlemise ja korduskatsete loogika
    max_retries = 5
    retry_delay = 60
    for attempt in range(max_retries):
        try:
            reply = get_completion(prompt, context)
            
            # √úlej√§√§nud funktsioon on sama...
            print("\n" + "="*80)
            print(f"ü§ñ MUDELI VASTUS s√µnale '{word}' t√§henduses '{definition[:50]}{'...' if len(definition) > 50 else ''}':")
            print("="*80)
            print(reply)
            print("="*80)
            
            safe_word = sanitize_filename(word)
            safe_definition = sanitize_filename(definition)
            out_path = os.path.join(OUTPUT_FOLDER, f"{safe_word}_{safe_definition}_analysis.txt")
            with open(out_path, "w", encoding="utf-8") as out_f:
                out_f.write(reply)
            
            parsed_result = parse_definition_analysis_response(reply, word, definition)
            
            print(f"‚úÖ {word} (t√§hendus: {definition[:30]}{'...' if len(definition) > 30 else ''}) ‚Äî Anal√º√ºs l√µpetatud\n")
            return parsed_result

        except exceptions.ResourceExhausted as e:
            print(f"‚ö†Ô∏è Viga 429: Limiit √ºletatud. Ootan {retry_delay} sekundit. Katse {attempt + 1}/{max_retries}.")
            time.sleep(retry_delay)
            retry_delay *= 2
        
        except Exception as e:
            print(f"‚ùå Ootamatu viga s√µnaga {word}, t√§hendus {definition}: {e}")
            return None
            
    print(f"‚ùå S√µna '{word}' t√∂√∂tlemine eba√µnnestus p√§rast {max_retries} katset. Liigun edasi.")
    return None


# --- P√µhiprogramm ---
def main():
    # See funktsioon ei vaja muudatusi
    all_rows = []
    
    input_file = "sisend.tsv"
    
    if not os.path.exists(input_file):
        print(f"‚õî Sisend fail '{input_file}' puudub!")
        return
    
    with open(input_file, newline="", encoding="utf-8") as csvfile:
        reader = csv.reader(csvfile, delimiter='\t')
        try:
            next(reader, None)
        except StopIteration:
            pass
        word_definitions = []
        for row in reader:
            if len(row) >= 2 and row[0].strip() and row[1].strip():
                word_definitions.append((row[0].strip(), row[1].strip()))
    
    for i, (word, definition) in enumerate(word_definitions, 1):
        print(f"\n{'='*60}")
        print(f"üìù ANAL√ú√úSIN ({i}/{len(word_definitions)}): '{word}' - '{definition[:50]}{'...' if len(definition) > 50 else ''}'")
        print(f"{'='*60}")
        
        result = process_definition_analysis(word, definition)
        if result:
            all_rows.append(result)
        else:
            print(f"‚ö†Ô∏è Lisame t√ºhja rea j√§rjekorra s√§ilitamiseks")
            all_rows.append({
                "S√µna": word, "T√§hendus": definition, "Tekstiregister": "ei m√§√§ratletud",
                "Registri p√µhjendus": "viga t√∂√∂tlemisel", "Treeningandmete p√µhjendus": "ei saadaval",
                "T√§henduste arv kokku": 0, "Sagedus": "ei m√§√§ratletud",
                "N√§ited": "ei saadaval", "Registrim√§rk": "ei kohaldu",
                "M√§rgendi p√µhjendus": "ei saadaval"
            })
        
        time.sleep(0.5)

    fieldnames = [
        "S√µna", "T√§hendus", "Tekstiregister", "Registri p√µhjendus", 
        "Treeningandmete p√µhjendus", "T√§henduste arv kokku", "Sagedus", 
        "N√§ited", "Registrim√§rk", "M√§rgendi p√µhjendus"
    ]

    with open(FINAL_CSV, "w", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames, delimiter=";", quoting=csv.QUOTE_ALL)
        writer.writeheader()
        if all_rows:
            writer.writerows(all_rows)

    print(f"\n‚úÖ L√µplik fail salvestatud: {FINAL_CSV}")

if __name__ == "__main__":
    main()